{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#general & system\\n\",\n",
    "import os\n",
    "\n",
    "#data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = 10, 10  #default setting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#data augmentation\n",
    "from PIL import Image\n",
    "from random import choice\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import keras.preprocessing.image as prep\n",
    "\n",
    "#ML part\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing import image as kimage\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalMaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D,Dropout,Activation,Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc,classification_report,roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc,classification_report,roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 5.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meta = pd.read_csv(\"../data/label_learn.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000000.jpg</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000002.jpg</td>\n",
       "      <td>malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000006.jpg</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000008.jpg</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000009.jpg</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name      label\n",
       "0  0000000.jpg     benign\n",
       "1  0000002.jpg  malignant\n",
       "2  0000006.jpg     benign\n",
       "3  0000008.jpg     benign\n",
       "4  0000009.jpg     benign"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 3.86 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_list = os.listdir(\"../data/resized_train//\")\n",
    "len(train_list) == len(meta.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 s, sys: 1.69 s, total: 23.7 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = np.array([np.array(Image.open(\"../data/resized_train/\"+fname)) for fname in meta.name])\n",
    "Y_train = [1 if x==\"malignant\" else 0 for x in meta.label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hair removal : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We proceed following the steps :\n",
    "\n",
    "- Data acquisition : Select a subset of data with Hair\n",
    "- RGB to Grayscale conversion â€“ Contrast Enhancement: Convert to Gray scale\n",
    "- Binarization : Binarize the Image : Convert the image to binary using an adaptive threshold\n",
    "- Edge Detection : Using Canny edge detector algorithm\n",
    "- Removal non-hair edges : \n",
    "- Hair mask creation : \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split for validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
    "Data_sets = []\n",
    "for train_index, test_index in skf.split(X_train, Y_train):\n",
    "    x_train, x_valid = X_train[train_index], X_train[test_index]\n",
    "    y_train, y_valid = Y_train[train_index], Y_train[test_index]\n",
    "    Data_sets.append([x_train, x_valid, y_train, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(Data_sets[0][0])\n",
    "Y_train = np.array(Data_sets[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = sum(Y_train)\n",
    "sample_size = temp*2700/(374+254)  #1372\n",
    "sample_size\n",
    "def random_undersampling(X, Y,sample_size = sample_size, n_sample=1):\n",
    "    \"\"\"\n",
    "    X,y : numpy arrays\n",
    "    return :\n",
    "    5 random ensemble of indices general_balenced_set:\n",
    "        general_balenced_set[0] = the shuffeled indices that inssure the class balance\n",
    "    \"\"\"\n",
    "    indices = np.array(range(len(Y)))\n",
    "    positive_samples = indices[Y==1]\n",
    "    #print(type(positive_samples))\n",
    "    negative_samples = indices[Y==0]\n",
    "    #print(type(negative_samples))\n",
    "    general_balenced_set = []\n",
    "    for k in range(n_sample):\n",
    "        indices_ = np.random.choice(negative_samples, sample_size, replace=False)\n",
    "        #print(len(indices_))\n",
    "        # append positive and negative\n",
    "        balenced_set = np.append(indices_, positive_samples)\n",
    "        #print(len(balenced_set))\n",
    "        # shuffle indices\n",
    "        np.random.shuffle(balenced_set)\n",
    "        #print(len(balenced_set))\n",
    "        general_balenced_set.append(balenced_set)\n",
    "    return general_balenced_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "general_balenced_set = random_undersampling(X_train,Y_train)\n",
    "X_train = X_train[general_balenced_set]\n",
    "Y_train = Y_train[general_balenced_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    [w_0, w_1] : weight of 0 class and weight of 1 class\n",
    "    \"\"\"\n",
    "    weights = K.variable(weights)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        #y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights[1] + (1 - y_true) * K.log(1 - y_pred) * weights[0]\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight_or = np.array([1.8,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss1 = weighted_categorical_crossentropy(np.array([1.8,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics to print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define  Deep Learing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_path = '../model/checkpoints/'\n",
    "    \n",
    "if not os.path.exists(weights_path):\n",
    "    os.makedirs(weights_path)\n",
    "    \n",
    "epochs = 500\n",
    "batch_size = 50\n",
    "\n",
    "#load base model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(299,299,3))\n",
    "#adding top layers\n",
    "    #sequentialy\n",
    "add_model = Sequential()\n",
    "add_model.add(GlobalAveragePooling2D(input_shape=base_model.output_shape[1:])) #Flatten/GlobalAveragePooling2D\n",
    "add_model.add(Dense(1024, activation='relu'))\n",
    "add_model.add(Dropout(0.25))\n",
    "add_model.add(Dense(512, activation='relu'))\n",
    "add_model.add(Dropout(0.25))\n",
    "add_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "\n",
    "\n",
    "#freeze lower layers of the model\n",
    "#for layer in model.layers[:]:\n",
    "    #layer.trainable = False\n",
    "for layer in model.layers[0:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss=loss1, #binary_crossentropy\n",
    "              metrics=[sensitivity, specificity,\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 72 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Lets define the image transormations that we wan\n",
    "\n",
    "gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         width_shift_range=0.2,\n",
    "                         height_shift_range=0.2,\n",
    "                         zoom_range=0.2,\n",
    "                         rotation_range=40)\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_one_input(X1, y):\n",
    "    genX1 = gen.flow(X1, y, batch_size=batch_size, seed=42)\n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        yield X1i[0], X1i[1]\n",
    "\n",
    "def val_datagen_(X1, y):\n",
    "    genX1 = val_datagen.flow(X1, y, batch_size=batch_size, seed=42)\n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        yield X1i[0], X1i[1]\n",
    "#Finally create out generator\n",
    "gen_flow_train = gen_flow_for_one_input(X_train, Y_train)\n",
    "#gen_flow_val = gen_flow_for_one_input(X_valid, Y_valid)\n",
    "gen_flow_val = val_datagen_(Data_sets[0][1], Data_sets[0][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 0.3763 - sensitivity: 0.6575 - specificity: 0.9664 - acc: 0.9410Epoch 00001: val_loss improved from inf to 0.62050, saving model to ../model/checkpoints/BestKerasResnet50_flat_3_loss.h5\n",
      "85/85 [==============================] - 87s 1s/step - loss: 0.3779 - sensitivity: 0.6545 - specificity: 0.9668 - acc: 0.9409 - val_loss: 0.6205 - val_sensitivity: 0.0341 - val_specificity: 0.9519 - val_acc: 0.9333\n",
      "Epoch 2/500\n",
      "43/85 [==============>...............] - ETA: 29s - loss: 0.3455 - sensitivity: 0.7532 - specificity: 0.9558 - acc: 0.9377"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "sensitivity = []\n",
    "specificty = []\n",
    "accuracy = []\n",
    "\n",
    "for [X_train, X_valid, y_train, y_valid] in Data_sets: \n",
    "    gen_flow_train = gen_flow_for_one_input(X_train, y_train)\n",
    "    epochs_to_wait_for_improve = 5\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=epochs_to_wait_for_improve)\n",
    "    checkpoint_callback = ModelCheckpoint(weights_path + 'BestKerasResnet50_flat_3_loss.h5', monitor='val_loss',\n",
    "                                          verbose=1, save_best_only=True, mode='min')\n",
    "    #fit the model\n",
    "    model.fit_generator(gen_flow_train, validation_data=(X_valid, y_valid),\n",
    "                        steps_per_epoch=int(np.ceil(len(X_train)/batch_size)),\n",
    "                        epochs=500, verbose=1, callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "                        class_weight = class_weight_or)\n",
    "\n",
    "    #evaluate the model\n",
    "    scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n",
    "    sensitivity.append(scores[0] * 100)\n",
    "    specificity.append(scores[1] * 100)\n",
    "    accuracy.append(scores[2] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
