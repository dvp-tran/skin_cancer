{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Cancer Classification Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary : ** skin cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#general & system\\n\",\n",
    "import os\n",
    "\n",
    "#data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = 10, 10  #default setting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#data augmentation\n",
    "from PIL import Image\n",
    "from random import choice\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import keras.preprocessing.image as prep\n",
    "\n",
    "#ML part\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing import image as kimage\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalMaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D,Dropout,Activation,Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc,classification_report,roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc,classification_report,roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Rq : ** Datasets are quite light, can be fully loaded in a laptop memory with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "meta = pd.read_csv(\"../data/label_learn.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_list = os.listdir(\"../data/resized_train/\")\n",
    "len(train_list) == len(meta.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list.sort() == list(meta.name).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = np.array([np.array(Image.open(\"../data/resized_train/\"+fname)) for fname in meta.name])\n",
    "Y_train = [1 if x==\"malignant\" else 0 for x in meta.label]\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(len(Y_train))\n",
    "X_train.shape[0] == len(meta.name) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#control\n",
    "print(meta.name[0:5])\n",
    "print(Y_train[0:5])\n",
    "print(meta.label[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Class weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight_or = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data splitting\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, random_state=42, train_size=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Resampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = sum(Y_train)\n",
    "sample_size = temp*1372/(374+254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_undersampling(X, Y,sample_size = sample_size, n_sample=1):\n",
    "    \"\"\"\n",
    "    X,y : numpy arrays\n",
    "    return :\n",
    "    5 random ensemble of indices general_balenced_set:\n",
    "        general_balenced_set[0] = the shuffeled indices that inssure the class balance\n",
    "    \"\"\"\n",
    "    indices = np.array(range(len(Y)))\n",
    "    positive_samples = indices[Y==1]\n",
    "    #print(type(positive_samples))\n",
    "    negative_samples = indices[Y==0]\n",
    "    #print(type(negative_samples))\n",
    "    general_balenced_set = []\n",
    "    for k in range(n_sample):\n",
    "        indices_ = np.random.choice(negative_samples, sample_size, replace=False)\n",
    "        #print(len(indices_))\n",
    "        # append positive and negative\n",
    "        balenced_set = np.append(indices_, positive_samples)\n",
    "        #print(len(balenced_set))\n",
    "        # shuffle indices\n",
    "        np.random.shuffle(balenced_set)\n",
    "        #print(len(balenced_set))\n",
    "        general_balenced_set.append(balenced_set)\n",
    "    return general_balenced_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "general_balenced_set = random_undersampling(X_train,Y_train)\n",
    "X_train = X_train[general_balenced_set]\n",
    "Y_train = Y_train[general_balenced_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Deep Learning with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_path = '../model/checkpoints/'\n",
    "\n",
    "if not os.path.exists(weights_path):\n",
    "    os.makedirs(weights_path)\n",
    "    \n",
    "epochs = 500\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Using RESNET : **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load base model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(299,299,3))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding top layers\n",
    "    #sequentialy\n",
    "add_model = Sequential()\n",
    "add_model.add(GlobalAveragePooling2D(input_shape=base_model.output_shape[1:])) #Flatten/GlobalAveragePooling2D\n",
    "add_model.add(Dense(1024, activation='relu'))\n",
    "add_model.add(Dropout(0.25))\n",
    "add_model.add(Dense(512, activation='relu'))\n",
    "add_model.add(Dropout(0.25))\n",
    "add_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#adding top layers\n",
    "    #need to check api to add layers sequentialy\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) #Flatten instead?\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    [w_0, w_1] : weight of 0 class and weight of 1 class\n",
    "    \"\"\"\n",
    "    weights = K.variable(weights)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        #y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights[1] + (1 - y_true) * K.log(1 - y_pred) * weights[0]\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss1 = weighted_categorical_crossentropy(class_weight_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#freeze lower layers of the model\n",
    "#for layer in model.layers[:]:\n",
    "    #layer.trainable = False\n",
    "for layer in model.layers[0:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss=loss1, #binary_crossentropy\n",
    "              metrics=[sensitivity, specificity,\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Lets define the image transormations that we wan\n",
    "\n",
    "gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         width_shift_range=0.2,\n",
    "                         height_shift_range=0.2,\n",
    "                         zoom_range=0.2,\n",
    "                         rotation_range=40)\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_one_input(X1, y):\n",
    "    genX1 = gen.flow(X1, y, batch_size=batch_size, seed=42)\n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        yield X1i[0], X1i[1]\n",
    "\n",
    "def val_datagen_(X1, y):\n",
    "    genX1 = val_datagen.flow(X1, y, batch_size=batch_size, seed=42)\n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        yield X1i[0], X1i[1]\n",
    "#Finally create out generator\n",
    "gen_flow_train = gen_flow_for_one_input(X_train, Y_train)\n",
    "#gen_flow_val = gen_flow_for_one_input(X_valid, Y_valid)\n",
    "gen_flow_val = val_datagen_(X_valid, Y_valid)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "epochs_to_wait_for_improve = 50\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=epochs_to_wait_for_improve)\n",
    "checkpoint_callback = ModelCheckpoint(weights_path + 'BestKerasResnet50_flat_2_loss.h5', monitor='val_loss',\n",
    "                                      verbose=1, save_best_only=True, mode='min')\n",
    "#fit the model\n",
    "model.fit_generator(gen_flow_train, validation_data=(X_valid, Y_valid),\n",
    "                    steps_per_epoch=int(np.ceil(len(X_train)/batch_size)),\n",
    "                    epochs=500, verbose=1, callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "                    class_weight = class_weight_or)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.fit_generator(gen_flow_train, validation_data=(X_valid, Y_valid) #gen_flow_val,  #(X_valid, Y_valid),\n",
    "                    steps_per_epoch=int(np.ceil(len(X_train)/batch_size)),\n",
    "                    #validation_steps=int(np.ceil(len(X_valid)/batch_size)),\n",
    "                    epochs=500, verbose=1, callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "                    class_weight = class_weight)\n",
    "                    #class_weights = class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Visualize  TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "# visualize the points' distribution of train data using features built from base_model\n",
    "import matplotlib.pyplot as plt\n",
    "# dimension reduction\n",
    "from sklearn.manifold import TSNE \n",
    "import numpy as np\n",
    "\n",
    "def t_sne_vis(name, base_model, x_processed_images, random_state, labels):\n",
    "    \"\"\"\n",
    "    :param name: the name of the cnn model used to build features\n",
    "    :param base_model: the model obj\n",
    "    :param x_processed_images: the input images for our model\n",
    "    :param random_state: for fixing the results\n",
    "    :param labels: 0/1 classification labels\n",
    "    :return:\n",
    "    the graph of image distribution based on features extracted from the model and the t-sne features\n",
    "    \"\"\"\n",
    "    # convert data to images\n",
    "    print('Converting data points to composite image')\n",
    "    X_train = x_processed_images\n",
    "    print('we got %d different images of shape %dx%d ' % (len(X_train), X_train.shape[1], X_train.shape[1]))\n",
    "    print('build usefull features from the selected model')\n",
    "    features = base_model.predict(X_train)\n",
    "    x_data1 = np.asarray(features).astype('float64')\n",
    "    x_data1 = x_data1.reshape((x_data1.shape[0], -1))\n",
    "    # perform t-SNE embedding\n",
    "    print('performing t-sne reduction')\n",
    "    vis_data = TSNE(perplexity=100, random_state=random_state).fit_transform(x_data1)\n",
    "    # plot the result\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    vis_x = vis_data[:, 0]\n",
    "    vis_y = vis_data[:, 1]\n",
    "    plt.scatter(vis_x, vis_y, c=labels, cmap=plt.cm.get_cmap(\"winter\", 2))\n",
    "    plt.colorbar(ticks=range(2))\n",
    "    plt.clim(0, 1)\n",
    "    plt.title(name)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig('tsne_vis_'+name+'.png')\n",
    "    return vis_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'Resnet'\n",
    "print('visualizing for %s' %name)\n",
    "base_model_ = Model(input=model.input, output=model.get_layer('avg_pool').output)\n",
    "tsne_feats = t_sne_vis(name, base_model_, X_valid, np.random.RandomState(42), Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(weights_path+\"BestKerasResnet50_flat_2_loss.h5\", custom_objects={'sensitivity': sensitivity,\n",
    "                                                                             \"specificity\":specificity,\n",
    "                                                                              \"loss1\":loss1})\n",
    "score = model.evaluate(X_valid, Y_valid, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test sensitivity:', score[1])\n",
    "print('Test specificity:', score[2])\n",
    "print('Test acc:', score[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute metrics\n",
    "pred = model.predict(X_valid)\n",
    "average_precision = average_precision_score(Y_valid, pred)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(Y_valid, pred)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_results(y_test, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.show()\n",
    "    print('AUC: %f' % roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#compute log loss\n",
    "pred = model.predict(X_valid)\n",
    "log_loss(Y_valid,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Retrain on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Generate the training data\n",
    "#Create 3 bands having HH, HV and avg of both\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis],\n",
    "                          ((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], axis=-1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#parse corrupted notebook file\n",
    "t = [\"#Generate the training data\\n\",\n",
    "    \"#Create 3 bands having HH, HV and avg of both\\n\",\n",
    "    \"X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\\\"band_1\\\"]])\\n\",\n",
    "    \"X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\\\"band_2\\\"]])\\n\",\n",
    "    \"X_train = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis],\\n\",\n",
    "    \"                          ((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\"]\n",
    "#t.remove(\"\\n\")\n",
    "t=[x.replace(\"\\n\",'') for x in t]\n",
    "for x in t:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
